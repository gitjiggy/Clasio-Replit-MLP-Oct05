import { storage } from '../storage.js';
import { analyzeDocumentContent } from '../gemini.js';
import type { BackgroundJob, NewBackgroundJob } from '../../shared/organizationSchema.js';
import { eq, and, or, isNull, lt } from 'drizzle-orm';

/**
 * Enhanced Enterprise Job Queue System üè¢‚ö°
 * 
 * Extends the existing AI queue processor to handle enterprise-grade background jobs:
 * - Multi-tenant job isolation
 * - Priority-based processing
 * - Retry logic with exponential backoff
 * - Job type extensibility
 * - Audit trail integration
 * 
 * Job Types:
 * - ai_analysis: Smart document analysis (preserves existing AI functionality)
 * - bulk_upload: Large file upload processing
 * - data_export: Organization data exports
 * - data_cleanup: Automated data lifecycle management
 * - audit_report: Compliance report generation
 * - user_invitation: Multi-tenant user invitations
 */

export interface JobProcessor {
  type: string;
  handler: (payload: any) => Promise<any>;
}

export class EnterpriseJobQueue {
  private isProcessing = false;
  private processingInterval: NodeJS.Timeout | null = null;
  private readonly PROCESSING_INTERVAL_MS = 10000; // Check every 10 seconds
  private readonly MAX_CONCURRENT_JOBS = 3; // Process up to 3 jobs simultaneously
  private readonly RETRY_DELAYS = [30000, 120000, 300000]; // 30s, 2m, 5m
  private currentJobCount = 0;
  
  // Job processors registry
  private processors: Map<string, JobProcessor> = new Map();

  constructor() {
    console.log('üè¢ Enterprise Job Queue initialized! Ready for multi-tenant background processing!');
    this.registerDefaultProcessors();
  }

  /**
   * Register built-in job processors
   */
  private registerDefaultProcessors(): void {
    // AI Analysis processor (preserves existing functionality)
    this.registerProcessor({
      type: 'ai_analysis',
      handler: this.processAIAnalysis.bind(this)
    });

    // Bulk upload processor
    this.registerProcessor({
      type: 'bulk_upload',
      handler: this.processBulkUpload.bind(this)
    });

    // Data export processor
    this.registerProcessor({
      type: 'data_export',
      handler: this.processDataExport.bind(this)
    });

    // Data cleanup processor
    this.registerProcessor({
      type: 'data_cleanup',
      handler: this.processDataCleanup.bind(this)
    });

    // Audit report processor
    this.registerProcessor({
      type: 'audit_report',
      handler: this.processAuditReport.bind(this)
    });

    console.log(`üìã Registered ${this.processors.size} job processors:`, Array.from(this.processors.keys()));
  }

  /**
   * Register a custom job processor
   */
  public registerProcessor(processor: JobProcessor): void {
    this.processors.set(processor.type, processor);
    console.log(`üîß Registered job processor: ${processor.type}`);
  }

  /**
   * Start the job queue processor
   */
  public start(): void {
    if (this.processingInterval) {
      console.log('‚è∏Ô∏è Enterprise Job Queue already running');
      return;
    }

    console.log('üöÄ Starting Enterprise Job Queue processor...');
    this.processingInterval = setInterval(() => {
      this.processJobs().catch(error => {
        console.error('‚ùå Error in job processing cycle:', error);
      });
    }, this.PROCESSING_INTERVAL_MS);

    // Start immediately
    this.processJobs().catch(error => {
      console.error('‚ùå Error in initial job processing:', error);
    });
  }

  /**
   * Stop the job queue processor
   */
  public stop(): void {
    if (this.processingInterval) {
      clearInterval(this.processingInterval);
      this.processingInterval = null;
      console.log('üõë Enterprise Job Queue processor stopped');
    }
  }

  /**
   * Add a job to the queue
   */
  public async enqueueJob(job: Omit<NewBackgroundJob, 'id' | 'createdAt' | 'updatedAt'>): Promise<string> {
    try {
      // Generate idempotency key if not provided
      const idempotencyKey = job.idempotencyKey || `${job.type}-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      
      const newJob: NewBackgroundJob = {
        ...job,
        idempotencyKey,
        payload: typeof job.payload === 'string' ? job.payload : JSON.stringify(job.payload),
      };

      const result = await storage.createBackgroundJob(newJob);
      
      console.log(`üì§ Job enqueued: ${result.type} (${result.id}) for org: ${result.organizationId || 'system'}`);
      return result.id;
    } catch (error) {
      console.error('‚ùå Failed to enqueue job:', error);
      throw error;
    }
  }

  /**
   * Main job processing loop
   */
  private async processJobs(): Promise<void> {
    if (this.isProcessing || this.currentJobCount >= this.MAX_CONCURRENT_JOBS) {
      return;
    }

    this.isProcessing = true;

    try {
      // Get pending jobs ordered by priority and scheduled time
      const pendingJobs = await storage.getPendingBackgroundJobs(this.MAX_CONCURRENT_JOBS);
      
      if (pendingJobs.length === 0) {
        return;
      }

      console.log(`üîÑ Processing ${pendingJobs.length} background jobs...`);

      // Process jobs concurrently
      const jobPromises = pendingJobs.map(job => this.processJob(job));
      await Promise.allSettled(jobPromises);

    } catch (error) {
      console.error('‚ùå Error in job processing loop:', error);
    } finally {
      this.isProcessing = false;
    }
  }

  /**
   * Process a single job
   */
  private async processJob(job: BackgroundJob): Promise<void> {
    this.currentJobCount++;
    
    try {
      console.log(`üéØ Processing job: ${job.type} (${job.id}) - Attempt ${job.attempts + 1}/${job.maxAttempts}`);

      // Mark job as processing
      await storage.updateBackgroundJobStatus(job.id, 'processing', { startedAt: new Date() });

      // Get processor for this job type
      const processor = this.processors.get(job.type);
      if (!processor) {
        throw new Error(`No processor found for job type: ${job.type}`);
      }

      // Parse payload
      let payload;
      try {
        payload = JSON.parse(job.payload);
      } catch (e) {
        payload = job.payload;
      }

      // Process the job
      const result = await processor.handler(payload);

      // Mark job as completed
      await storage.updateBackgroundJobStatus(job.id, 'completed', {
        completedAt: new Date(),
        result: JSON.stringify(result)
      });

      console.log(`‚úÖ Job completed successfully: ${job.type} (${job.id})`);

    } catch (error) {
      console.error(`‚ùå Job failed: ${job.type} (${job.id}):`, error);
      await this.handleJobFailure(job, error);
    } finally {
      this.currentJobCount--;
    }
  }

  /**
   * Handle job failure and retry logic
   */
  private async handleJobFailure(job: BackgroundJob, error: any): Promise<void> {
    const newAttempts = job.attempts + 1;
    const errorMessage = error instanceof Error ? error.message : String(error);

    if (newAttempts >= job.maxAttempts) {
      // Max attempts reached, mark as failed
      await storage.updateBackgroundJobStatus(job.id, 'failed', {
        errorMessage,
        completedAt: new Date()
      });
      console.log(`üíÄ Job failed permanently: ${job.type} (${job.id}) after ${newAttempts} attempts`);
    } else {
      // Schedule retry with exponential backoff
      const retryDelay = this.RETRY_DELAYS[Math.min(newAttempts - 1, this.RETRY_DELAYS.length - 1)];
      const scheduledFor = new Date(Date.now() + retryDelay);

      await storage.updateBackgroundJobStatus(job.id, 'pending', {
        attempts: newAttempts,
        errorMessage,
        scheduledFor
      });

      console.log(`üîÑ Job retry scheduled: ${job.type} (${job.id}) in ${retryDelay / 1000}s`);
    }
  }

  /**
   * AI Analysis processor (preserves existing functionality)
   */
  private async processAIAnalysis(payload: any): Promise<any> {
    const { documentId, priority = 5 } = payload;

    if (!documentId) {
      throw new Error('Document ID is required for AI analysis');
    }

    // Get document details
    const document = await storage.getDocumentById(documentId);
    if (!document) {
      throw new Error(`Document not found: ${documentId}`);
    }

    console.log(`ü§ñ Processing AI analysis for document: ${document.name}`);

    // Perform AI analysis using existing gemini service
    const analysis = await analyzeDocumentContent(document.documentContent || '');

    // Update document with AI results
    await storage.updateDocument(documentId, {
      aiSummary: analysis.conciseTitle,
      aiKeyTopics: analysis.keyTopics,
      aiDocumentType: analysis.documentType,
      aiCategory: analysis.category,
      aiSentiment: 'neutral', // Default sentiment since not provided by current API
      aiWordCount: analysis.wordCount,
      aiAnalyzedAt: new Date(),
      aiConciseName: analysis.conciseTitle,
      aiCategoryConfidence: analysis.categoryConfidence,
      aiDocumentTypeConfidence: analysis.documentTypeConfidence,
    });

    console.log(`‚ú® AI analysis completed for: ${document.name}`);
    return { documentId, analysis: 'completed' };
  }

  /**
   * Bulk upload processor
   */
  private async processBulkUpload(payload: any): Promise<any> {
    const { uploadBatch, organizationId } = payload;
    
    console.log(`üì¶ Processing bulk upload: ${uploadBatch.length} files for org: ${organizationId}`);
    
    const results = {
      total: uploadBatch.length,
      successful: 0,
      failed: 0,
      errors: [] as string[]
    };

    for (const file of uploadBatch) {
      try {
        // Process each file in the batch
        // This would integrate with your existing bulk upload logic
        console.log(`üìÑ Processing file: ${file.name}`);
        results.successful++;
      } catch (error) {
        results.failed++;
        results.errors.push(`${file.name}: ${error}`);
      }
    }

    console.log(`üì¶ Bulk upload completed: ${results.successful}/${results.total} successful`);
    return results;
  }

  /**
   * Data export processor
   */
  private async processDataExport(payload: any): Promise<any> {
    const { organizationId, exportType, dateRange, includeDeleted = false } = payload;
    
    console.log(`üìä Processing data export for org: ${organizationId}, type: ${exportType}`);
    
    // This would implement the actual export logic
    // For now, return a placeholder
    return {
      organizationId,
      exportType,
      status: 'completed',
      recordCount: 0,
      exportUrl: `exports/${organizationId}/${Date.now()}.${exportType}`
    };
  }

  /**
   * Data cleanup processor
   */
  private async processDataCleanup(payload: any): Promise<any> {
    const { organizationId, retentionDays } = payload;
    
    console.log(`üßπ Processing data cleanup for org: ${organizationId}, retention: ${retentionDays} days`);
    
    // This would implement actual cleanup logic
    return {
      organizationId,
      deletedDocuments: 0,
      archivedDocuments: 0,
      cleanupDate: new Date()
    };
  }

  /**
   * Audit report processor
   */
  private async processAuditReport(payload: any): Promise<any> {
    const { organizationId, reportType, dateRange } = payload;
    
    console.log(`üìã Processing audit report for org: ${organizationId}, type: ${reportType}`);
    
    // This would implement actual audit report generation
    return {
      organizationId,
      reportType,
      status: 'completed',
      recordCount: 0,
      reportUrl: `reports/${organizationId}/${Date.now()}-${reportType}.pdf`
    };
  }

  /**
   * Get job statistics for monitoring
   */
  public async getJobStats(): Promise<any> {
    return await storage.getBackgroundJobStats();
  }

  /**
   * Cancel a specific job
   */
  public async cancelJob(jobId: string): Promise<void> {
    await storage.updateBackgroundJobStatus(jobId, 'cancelled');
    console.log(`üö´ Job cancelled: ${jobId}`);
  }

  /**
   * Get job history for an organization
   */
  public async getJobHistory(organizationId: string, limit = 100): Promise<BackgroundJob[]> {
    return await storage.getBackgroundJobHistory(organizationId, limit);
  }
}

// Export singleton instance
export const enterpriseJobQueue = new EnterpriseJobQueue();